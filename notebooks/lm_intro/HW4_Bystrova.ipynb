{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc745264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1534fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#применяем нормализацию к текстам\n",
    "norm_news = normalize(news)\n",
    "\n",
    "#cоздаем частотные словари\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "838eaad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 72412),\n",
       " ('и', 33290),\n",
       " ('на', 28434),\n",
       " ('по', 19490),\n",
       " ('что', 17031),\n",
       " ('с', 15921),\n",
       " ('не', 12702),\n",
       " ('из', 7727),\n",
       " ('о', 7515),\n",
       " ('как', 7514)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f37a158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 0.04808907489694771),\n",
       " ('и', 0.0221080111489724),\n",
       " ('на', 0.018883123731146926),\n",
       " ('по', 0.012943380513471676),\n",
       " ('что', 0.011310349590812525),\n",
       " ('с', 0.01057319451795703),\n",
       " ('не', 0.008435444806676101),\n",
       " ('из', 0.005131529052211166),\n",
       " ('о', 0.00499073907433246),\n",
       " ('как', 0.0049900749706632205)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#переводим абсолютные частоты в вероятности, разделив количество употреблений слова на общее число слов в корпусе\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "141e6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>' + \"\"] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "88e9981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем отрывок из 50 предложений для расчета перплексии\n",
    "\n",
    "test_news = sentences_news[-50:]\n",
    "\n",
    "# убираем эти предложения из основной выборки\n",
    "\n",
    "sentences_news = sentences_news[:-50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "dbec0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ищем кол-во вхождений каждой n-граммы и проходимся по токенам\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d41a4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем частотные словари униграм, биграм и триграм\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d5458498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> об этом', 1578),\n",
       " ('<start> по словам', 1549),\n",
       " ('сообщает риа новости', 1324),\n",
       " ('со ссылкой на', 1242),\n",
       " ('риа новости <end>', 1228),\n",
       " ('<start> кроме того', 1070),\n",
       " ('<start> как сообщает', 1064),\n",
       " ('<start> напомним что', 1005),\n",
       " ('по его словам', 899),\n",
       " ('<start> по его', 868)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c83cf70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> в', 7960),\n",
       " ('<start> по', 6210),\n",
       " ('<start> как', 3736),\n",
       " ('риа новости', 3502),\n",
       " ('по словам', 1971),\n",
       " ('об этом', 1794),\n",
       " ('<start> однако', 1693),\n",
       " ('<start> на', 1642),\n",
       " ('что в', 1621),\n",
       " ('<start> об', 1618)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ae69c",
   "metadata": {},
   "source": [
    "### Генерация текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d78e37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем матрицу вероятностей для новостей\n",
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    w1, w2, w3 = ngram.split()\n",
    "    bigram = w1 + ' ' + w2\n",
    "    trigram = w1 + ' ' + w2 + ' ' + w3\n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word3]] =  (trigrams_news[trigram]/\n",
    "                                                                bigrams_news[bigram])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, bigram2id, id2bigram, n=250, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = id2bigram[start]\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = matrix[current_idx].argmax()\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = bigram2id[start]\n",
    "            \n",
    "        else:\n",
    "            bigram = id2bigram[current_idx]\n",
    "            trigram = bigram + ' ' + id2word[chosen]\n",
    "            w1, w2, w3 = trigram.split()\n",
    "            new_bigram = w2 + ' ' + w3\n",
    "            chosen = bigram2id[new_bigram]\n",
    "   \n",
    "        \n",
    "        current_idx = chosen\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df677d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(generate(matrix_news, id2word_news, id2bigram_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f21a41",
   "metadata": {},
   "source": [
    "### Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1d7013ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b16f817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = []\n",
    "\n",
    "for sentence in test_news[:15]:\n",
    "    prob = []\n",
    "\n",
    "    \n",
    "    for ngram in ngrammer(sentence, n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + ' ' + word2\n",
    "        \n",
    "        if gram2 in bigrams_news and ngram in trigrams_news:        \n",
    "            prob.append(np.log(trigrams_news[ngram] / bigrams_news[gram2]))\n",
    "        else:\n",
    "            prob.append(np.log(0.00001))\n",
    "    \n",
    "    perplexities.append(perplexity(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "36c95680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1476.009465923114,\n",
       " 10307.66269203421,\n",
       " 27045.318696275972,\n",
       " 99999.99999999991,\n",
       " 23134.59701913287,\n",
       " 17443.65142811791,\n",
       " 30493.357264575407,\n",
       " 16614.712259318607,\n",
       " 12956.68420148759,\n",
       " 8994.822584971907,\n",
       " 143.8539784980612,\n",
       " 99999.99999999991,\n",
       " 425.0215501554316,\n",
       " 183.14458224906917,\n",
       " 20670.11216429356]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "68f196ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24659.263192468898"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(perplexities)  ##усредненная перплексия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e30ba99",
   "metadata": {},
   "source": [
    "В книге представлены ещё 2 варианта обработки несловарных слов, используя тэг <UNK>:\n",
    "1. По заранее созданному словарю\n",
    "    1.1. Выбрать фиксированный список слов (словарь);\n",
    "    1.2. На этапе нормализации текста в обучающей выборке преобразовать все несловарные слова (т.е. не входящее в эту выборку) в токены неизвестных слов <UNK>.\n",
    "    1.3.  Рассчитать вероятности для <UNK> как для других слов в обучающей выборке.\n",
    "\n",
    "2. Когда такого заранее заданного словаря нет, можно создать его неявно, заменяя слова с низкой частотностью (вручную задать границу N, ниже которой слова считаются неизвестными, или заранее задать максимальный размер словаря V и выбрать верхние топ-V слов по частотности, а остальные признать неизвестными) на тэг <UNK>.\n",
    "    \n",
    "    В любом случае тэг <UNK> рассматривается и опертируется как обычный токен.\n",
    "    \n",
    "    Точный выбор <UNK> модели влияет на метрику перплексии. Языковая модель может добиться низкого уровня перплексии, выбирая небольшой словарь и присваивая несловарным словами высокую вероятность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aeca9a1b",
   "metadata": {},
   "source": [
    "    Бывают ситуации, когда слова, которые есть в нашем словаре, но вдруг появляются в тестовом наборе данных в неизвестном контексте (например, следуют за словом, после которого они никогда не появлялись в обучающей выборке). Чтобы языковая модель не зануляла вероятности таким словам, предлагается снизить немного вероятность наиболее частотных слов и присвоить ее событиям, которые нам не встречались ранее, таким образом распределяя вероятность между переменными. Это называется сглаживанием. \n",
    "    Существуют различные способы сглаживания: \n",
    "    ● Сглаживание Лапласа (add-one)\n",
    "    ● add-k сглаживание\n",
    "    ● Сглаживание Кнесера-Нея (Kneser-Ney)\n",
    "    ● Откат (backoff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
